---
title: "Hayden and Ying's R tutorial"
author: "Hayden Wainwright and Ying Chen, Lougheed Lab, Queen's University"
date: "04/30/2020"
output:
  html_document: default
  fig_caption: default
---

<style type="text/css">
h1.title {
  font-family: "Times New Roman", Times, serif;
  text-align: center;
}
h4.author { /* Header 4 - and the author and data headers use this too  */
  font-family: "Times New Roman", Times, serif;
  text-align: center;
}
h4.date { /* Header 4 - and the author and data headers use this too  */
  font-family: "Times New Roman", Times, serif;
  text-align: center;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
```

## Preface

In <font size="4" color="red">2020</font>, a pandemic caused by the  <font size="4" color="blue">COVID-19</font> virus hit human society harshly, with <font size="4" color="dark blue">million </font> people infected and <font size="4" color="dark blue">thousands of</font> people dead. As simple country <font size="4" color="orange">biologists</font>, we are staying home at the behest of Government of Canada, helping *Homo sapiens* maintain good  <font size="5" color="green">health</font> and congratulating <font size="5" color="green">nature</font> for a temporary break from human acitivties. 

As the rock below reminds us <font size="5">**Don't give up!**</font>. We will continue our <font size="4" color="brown">academic</font> pursuits and brain-hearting <font size="4" color="brown">science</font> work in Lougheed Lab at Queen's University. We decided today is the day, to actually do some <font size="4" color="blue">R</font> work. Follow **Hayden** and **Ying** and do today's **brain exercise**. We will go through:  


- Section 1: R basics
- Section 2: Data simulation 
- Section 3: Power analysis 
- Section 4: Data visulization 

We have a real <font size="4" color="orange">biological question</font> to be solved: 

- Has frog mortality changed in current COVID-19 situation in Ontario compared to before?

Two <font size="4" color="orange">hypotheses</font> raised: 

- H0: It does not change.
- H1: Frog mortality rate decreases with decreasing traffic under Ontario lockdown. 

<font size="4" color="blue">R</font>eady? Let's Go!

![<font size="4" color="gray">*You are my rock! Love from Lougheed Lab. Photo by La Vita E Bella.*</font>](IMG_6184.JPG)


<p>&nbsp;</p>

## Section 1: R Basics (5-10min tops) (Hayden):
1. how to make a project
2. markdown vs script
3. github links
4. handling error messages 
        (dependancies missing, unexpect ",", googling answers)
5. how to google 101
6. other helpful links (Ying)
    + Quick-R: https://www.statmethods.net/index.html 
    + ggplot2: https://learnr.wordpress.com 
7. packages
    + You need to install: pwr, ggplot2, dplyr, knitr, kableExtra, dabestr


<p>&nbsp;</p>

## Section 2: Data simulation (30min) (Hayden):
1. how to generate a "fake" data set

  -(Frog mortality, frog species (3), location (3), sampling year (2010-2020), Total number of cars)
  -generate date for each year separately, then can change number of cars for this year specifically, 
  -combine the data from different years into one data frame
  -start with 1 species and 1 location, add more in if you can later
  

```{r}


Frog_Mort <- c((rnorm(n=90, mean = 26, sd = 1)), (rnorm(n = 9, mean = 24, sd = 1)))# assuming average frog mortality of 26% initially and then 24% after covid hit

Species <- c("Rana clamitans", "Pseudacris crucifer", "Lithobates pipiens")
Location <- c("Opinicon Road", "Hwy15", "Jones Falls")
#Location <- c("Opinicon Road", "Opinicon Road", "Opinicon Road", "Hwy15", "Hwy15", "Hyw15", "Jones Falls", "Jones Falls", "Jones Falls")
Year <- seq(2010, 2020, by=1)
Cars_Op <- rnorm(n=1, mean = 50, sd = 8)
Cars_H15 <- rnorm(n=1, mean = 60, sd = 8)
Cars_Falls <- rnorm(n=1, mean = 40, sd =8)
Cars <- c(Cars_Op, Cars_H15, Cars_Falls)

Year <- rep(Year, each = 9)
Species <- rep(Species, 11)
Location <- rep(Location, 9)
#Cars <- rep(Cars, )
#Now we can make our data frame!

Data1 <- cbind(Year, Location, Species, Frog_Mort)
Data1 <- data.frame(Data1)

head(Data1)




# Frog Mortality 

mor_mean_pool <- rnorm(n=10, mean=25, sd=1)
car_mean_pool <- rnorm(n=10, mean=50, sd=1)

for (i in 1:seq(2010,2019,1)){
  mean <- sample(mor_mean_pool, 1) 
  car <- sample(car_mean_pool, 1)
  
  Frog_Mort_op_2010 <- rnorm(n=3, mean = mean, sd=1)
  Frog_Mort_op_2010 <- sort(Frog_Mort) # smaller values at the beginning
  Cars_op_2010 <- rnorm(n=3, mean = 50, sd = 8) 
  Cars_op_2010 <- sort(Cars)
# Assign cars to frog mortality in a dataset
  dat_op_2010 <- data.frame(Frog_Mort_op_2010, Cars_op_2010) # small cars have small frog mortality 
# add locations to it
  dat_op_2010$Location <- rep("Opinicon Road",3)
  dat_op_2010$Year <- rep("2010",3)
  dat_op_2010$Species <- c("x","y","z")

  
}

mean <- sample(mor_mean_pool, 1) 
car <- sample(car_mean_pool, 1)
  
Frog_Mort_op_2010 <- rnorm(n=3, mean = mean, sd=1)
Frog_Mort_op_2010 <- sort(Frog_Mort) # smaller values at the beginning
Cars_op_2010 <- rnorm(n=3, mean = 50, sd = 8) 
Cars_op_2010 <- sort(Cars)
# Assign cars to frog mortality in a dataset
dat_op_2010 <- data.frame(Frog_Mort_op_2010, Cars_op_2010) # small cars have small frog mortality 
# add locations to it
dat_op_2010$Location <- rep("Opinicon Road",3)
dat_op_2010$Year <- rep("2010",3)
dat_op_2010$Species <- c("x","y","z")

View(dat_op_2010)

# then we create another 2 datasets with another 2 locations and year 2001 (the mean and sd for cars and frog mortality can be changed), we can merge them together. Year 2001 will be complete.
# then we repeat it to create another 10 years. And merge them all together. 
# We can probably write a loop to do this. 


```

Alternatively we can also build our data set this way:
  
  
```{r}
library(dplyr)

set.seed(54321)
 
N_Int = 90
N_COVID = 9
N_OpRoad = 33
N_Hyw15 = 33
N_Falls = 33
N = 5

C1 <- rnorm(N, mean = 25, sd = 25)
C2 <- rnorm(N, mean = 20, sd = 25)
species <- c(rep('Anaxyrus fowleri', N/3), rep('Pseudacris crucifer', N/3), rep('Rana pipiens', N/3))
dummy <- rep("dummy", N)
id <- 1: N
 
 
# tibble::tibble(
#    Before = C1, During = C2,
#    Dummy = dummy,
#    Species = species, ID = id)

#Data2 <- 
#  wide.data %>%
#  tidyr::gather(key = Group, value = Frog_Mortality, -ID, -Species, -Dummy)

library(dabestr)

#tow.group.unpaired <- my.data %>%
#  dabest(Group, Measurement, 
         # The idx below passes "Control" as the control group, 
         # and "Group1" as the test group. The mean difference
         # will be computed as mean(Group1) - mean(Control1).
#         idx = c("Pre-Pandemic", "Pandemic"), 
#         paired = FALSE)

#two.group.unpaired 

#plot(two.group.unpaired, color.column = Species)



```
  
  From website example:
  
```{r}
library(dplyr)

set.seed(54321)
#N = 10
N = 40
#N = 100
c1 <- rnorm(N, mean = 100, sd = 25)
c2 <- rnorm(N, mean = 100, sd = 50)
g1 <- rnorm(N, mean = 120, sd = 25)
g2 <- rnorm(N, mean = 80, sd = 50)
g3 <- rnorm(N, mean = 100, sd = 12)
g4 <- rnorm(N, mean = 100, sd = 50)
gender <- c(rep('Male', N/2), rep('Female', N/2))
dummy <- rep("Dummy", N)
id <- 1: N

wide.data <- 
  tibble::tibble(
    Control1 = c1, Control2 = c2,
    Group1 = g1, Group2 = g2, Group3 = g3, Group4 = g4,
    Dummy = dummy,
    Gender = gender, ID = id)


my.data   <- 
  wide.data %>%
  tidyr::gather(key = Group, value = Measurement, -ID, -Gender, -Dummy)


library(dabestr)

two.group.unpaired <- 
  my.data %>%
  dabest(Group, Measurement, 
         # The idx below passes "Control" as the control group, 
         # and "Group1" as the test group. The mean difference
         # will be computed as mean(Group1) - mean(Control1).
         idx = c("Control1", "Group1"), 
         paired = FALSE)

two.group.unpaired 

plot(two.group.unpaired, color.column = Gender)
```
  
  
  
2. how to export the data set

It may be useful, from time to time, to be able to save your data set that was created in R. To do this we simply can use the 'write.csv()' command as shown bellow for the first data set we created. 

```{r}
write.csv(Data1, 'FrogPandemic.csv')

```


3. power analysis






























<p>&nbsp;</p>

## Section 3. Power analysis (Ying)
#### 3.1 What is power analysis? 

<font size="2" color="red"> *The following materials are prepared by Ying upon her understanding, which means they can be flawed.* </font> 

<font size="2" color="red"> *Ying highly recommends you to read authoritative books and papers for learning. Here are some:* </font> 

- <font size="2" color="gray">*Cohen, J. 1988. Statistical Power Analysis for the Behavioral Sciences, Second Edition. Hillsdale, New Jersey: Lawrence Erlbaum Associates.*</font>
- <font size="2" color="gray">*Murphy, K. R. and Myors, B. 2004. Statistical Power Analysis: A Simple and General Model for Traditional and Modern Hypothesis Tests. Mahwah, New Jersey:  Lawrence Erlbaum Associates.*</font>
- <font size="2" color="gray">*Sedlmeier, P. and Gigerenzer, G. 1989. Do Studies of Statistical Power Have an Effect on the Power of Studies?  Psychological Bulletin, 105(2), 309-316.*</font>
- <font size="2" color="gray">*Hoenig, J.M., and D. M. Heisey. 2001. The Abuse of Power: The Pervasive Fallacy of Power Calculations for Data Analysis. The American Statistician, 55(1): 19-24.*</font>
- <font size="2" color="gray">*Levine, M., and Ensom M. H. H. 2001. Post Hoc Power Analysis: An Idea Whose Time Has Passed?  Pharmacotherapy, 21(4), 405-409.*</font>



<p>&nbsp;</p>
**Statistical power** is the probability that the test correctly rejects the null hypothesis.

- Power = Pr (Reject H0 | H1 is true) = 1 - Type II error rate

```{r, echo=FALSE,warning=FALSE,message=FALSE}
# How to make a nice table? Please check: https://cran.r-project.org/web/packages/kableExtra/vignettes/awesome_table_in_html.html 

library(knitr)
library(kableExtra)

PAcol1 <- c("", "Null hypothesis H0 is true", "Alternative hypothesis H1 is ture")
PAcol2 <- c("Fail to reject H0", "Correct", "Type II error")
PAcol3 <- c("Reject H0", "Type I error", "Correct. Power")
PAtable <- data.frame(PAcol1, PAcol2, PAcol3)
colnames(PAtable) <- NULL

kable(PAtable, caption="Table 1. Hypotheis testing.") %>%
  kable_styling(c("striped", "hover"), full_width = FALSE, position = "left")
```

Let's use a Z test on the population mean to recall hypothesis testing. 

Let's assume the true population mean frog mortality rate from Lake Opinicon is 24%. And the alternative hypothesis is that the true population mean is smaller than 24%. 

H0: μ = 24

H1: μ < 24

Let's visualize the hypothesis testing. The curve is the sampling distribution of $\bar{x}$ when the null hypothesis is true. Recall that $\bar{X} = \frac{1}{n} \sum_{i=1}^n X_i$
If your sample mean falls in the blue region that is smaller than 22.35, then you reject your hypothesis as your alpha value is set to 0.05. 

```{r,echo=FALSE,warning=FALSE,message=FALSE}
curve(dnorm(x,24,1), xlim=c(20,28), main="Normal density",axes=FALSE,xlab="",ylab="",col="blue",lwd=5)
axis(1,at=c(20,21,22,23,24,25,26,27,28))

# define shaded region
from.z <- 20
to.z <- qnorm(.05,mean=24,sd=1)

S.x  <- c(from.z, seq(from.z, to.z, 0.01), to.z)
S.y  <- c(0, dnorm(seq(from.z, to.z, 0.01),24,1), 0)
polygon(S.x,S.y, col="blue",border=NA)

text(x=21,y=0.1,labels=expression(paste(alpha, "=0.05")),col="darkblue")
text(x=25,y=0.37,labels=expression(paste(mu, "=24")),col="darkblue")
#text(x=22.8,y=0,labels=round(to.z,1))
abline(v=to.z,lty="dashed")
arrows(21,0.08,21.8,0.04)
text(x=21,y=0.2,labels="Reject H0")
text(x=24,y=0.2,labels="Do not reject H0")
```


Let's look at a curve when the alternative hypothesis is true, i.e. μ < 24. The power (turquoise area) is the probability of rejecting H0 if the true mean is 23. 


```{r,echo=FALSE,warning=FALSE,message=FALSE}
curve(dnorm(x,24,1), xlim=c(20,28), main="Normal density",axes=FALSE,xlab="",ylab="",col="blue",lwd=5)
curve(dnorm(x,23,1),add=TRUE,col="turquoise",lwd=5)
axis(1,at=c(20,21,22,23,24,25,26,27,28))

# define shaded region
from.z <- 20
to.z <- qnorm(.05,mean=24,sd=1)

S.x  <- c(from.z, seq(from.z, to.z, 0.01), to.z)
S.y1  <- c(0, dnorm(seq(from.z, to.z, 0.01),24,1), 0)
S.y2  <- c(0, dnorm(seq(from.z, to.z, 0.01),23,1), 0)
polygon(S.x,S.y2, col="turquoise",border=NA)
polygon(S.x,S.y1, col="blue",border=NA)

text(x=25,y=0.37,labels=expression(paste(mu, "=24")),col="darkblue")
text(x=24.2,y=0.3,labels=expression(paste(mu, "<24")),col="turquoise")
text(x=21.8,y=0.37,labels=expression(paste(alpha, "=0.05")))
abline(v=to.z,lty="dashed")
arrows(21,0.2,21.2,0.14)
text(x=21,y=0.23,labels="Power",size=5)
```



Power depends on:

- **Alpha level** = significance level = Type I error rate. Larger alpha, greater power.
    + Pr (observed | H0 is ture) < alpha, reject null 
    + Pr (observed | H0 is ture) > alpha, fail to reject null 

```{r,echo=FALSE,warning=FALSE,message=FALSE}
curve(dnorm(x,24,1), xlim=c(20,28), main="Normal density",axes=FALSE,xlab="",ylab="",col="blue",lwd=5)
curve(dnorm(x,23,1),add=TRUE,col="turquoise",lwd=5)
axis(1,at=c(20,21,22,23,24,25,26,27,28))

# define shaded region
from.z <- 20
to.z <- qnorm(.01,mean=24,sd=1)
to.z1 <- qnorm(.05,mean=24,sd=1)

S.x  <- c(from.z, seq(from.z, to.z, 0.01), to.z)
S.y1  <- c(0, dnorm(seq(from.z, to.z, 0.01),24,1), 0)
S.y2  <- c(0, dnorm(seq(from.z, to.z, 0.01),23,1), 0)
polygon(S.x,S.y2, col="turquoise",border=NA)
polygon(S.x,S.y1, col="blue",border=NA)

text(x=25,y=0.37,labels=expression(paste(mu, "=24")),col="darkblue")
text(x=24.2,y=0.3,labels=expression(paste(mu, "<24")),col="turquoise")
abline(v=to.z,lty="dashed")
text(x=21,y=0.37,labels=expression(paste(alpha, "=0.01")))
abline(v=to.z1,lty="dashed")
text(x=22.3,y=0.39,labels=expression(paste(alpha, "=0.05")))
#arrows(21,0.2,21.2,0.14)
#text(x=21,y=0.23,labels="Power",size=5)
```


- **Effect size** - magnitude of the difference between groups. Larger effects, larger power. Notice that there are many definitions of the effect size. We will stick with the simplest one, which is the difference of two group means. 


```{r,echo=FALSE,warning=FALSE,message=FALSE}
curve(dnorm(x,24,1), xlim=c(19,28), ylim=c(0,0.45), main="Normal density",axes=FALSE,xlab="",ylab="",col="blue",lwd=5)
curve(dnorm(x,22,1),add=TRUE,col="turquoise",lwd=5)
axis(1,at=c(19,20,21,22,23,24,25,26,27,28))
arrows(23,0.42,22,0.42)

# define shaded region
from.z <- 19
to.z <- qnorm(.05,mean=24,sd=1)

S.x  <- c(from.z, seq(from.z, to.z, 0.01), to.z)
S.y1  <- c(0, dnorm(seq(from.z, to.z, 0.01),24,1), 0)
S.y2  <- c(0, dnorm(seq(from.z, to.z, 0.01),22,1), 0)
polygon(S.x,S.y2, col="turquoise",border=NA)
polygon(S.x,S.y1, col="blue",border=NA)

curve(dnorm(x,23,1),add=TRUE,lty="dashed")

text(x=25,y=0.37,labels=expression(paste(mu, "=24")),col="darkblue")
text(x=23,y=0.43,labels=expression(paste(mu, "<24")),col="turquoise")
text(x=22.9,y=0.04,labels=expression(paste(alpha, "=0.05")))
abline(v=to.z,lty="dashed")
```




- **Sample size** - Larger sample size, smaller sampling error, larger statistical power. 
```{r,echo=FALSE,warning=FALSE,message=FALSE}

par(mfrow=c(3,1))
# sd=2
curve(dnorm(x,24,2), xlim=c(15,32),ylim=c(0,0.8), main="Normal density (sd=2)",axes=FALSE,xlab="",ylab="",col="blue",lwd=5)
curve(dnorm(x,22,2),add=TRUE,col="turquoise",lwd=5)
axis(1,at=c(15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32))

# define shaded region
from.z <- 15
to.z <- qnorm(.05,mean=24,sd=2)

S.x  <- c(from.z, seq(from.z, to.z, 0.01), to.z)
S.y1  <- c(0, dnorm(seq(from.z, to.z, 0.01),24,2), 0)
S.y2  <- c(0, dnorm(seq(from.z, to.z, 0.01),22,2), 0)
polygon(S.x,S.y2, col="turquoise",border=NA)
polygon(S.x,S.y1, col="blue",border=NA)

# sd=1
curve(dnorm(x,24,1), xlim=c(15,32),ylim=c(0,0.8), main="Normal density (sd=1)",axes=FALSE,xlab="",ylab="",col="blue",lwd=5)
curve(dnorm(x,23,1),add=TRUE,col="turquoise",lwd=5)
axis(1,at=c(15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32))

# define shaded region
from.z <- 15
to.z <- qnorm(.05,mean=24,sd=1)

S.x  <- c(from.z, seq(from.z, to.z, 0.01), to.z)
S.y1  <- c(0, dnorm(seq(from.z, to.z, 0.01),24,1), 0)
S.y2  <- c(0, dnorm(seq(from.z, to.z, 0.01),23,1), 0)
polygon(S.x,S.y2, col="turquoise",border=NA)
polygon(S.x,S.y1, col="blue",border=NA)


# sd=0.5
curve(dnorm(x,24,0.5), xlim=c(15,32), ylim=c(0,0.8), main="Normal density (sd=0.5)",axes=FALSE,xlab="",ylab="",col="blue",lwd=5)
curve(dnorm(x,23,0.5),add=TRUE,col="turquoise",lwd=5)
axis(1,at=c(15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32))

# define shaded region
from.z <- 15
to.z <- qnorm(.05,mean=24,sd=0.5)

S.x  <- c(from.z, seq(from.z, to.z, 0.01), to.z)
S.y1  <- c(0, dnorm(seq(from.z, to.z, 0.01),24,0.5), 0)
S.y2  <- c(0, dnorm(seq(from.z, to.z, 0.01),23,0.5), 0)
polygon(S.x,S.y2, col="turquoise",border=NA)
polygon(S.x,S.y1, col="blue",border=NA)
```


- **Your purpose** - For example, one might want to see if the regression coefficient is different from zero, while the other wants to get a very precise estimate of the regression coefficient with a very small confidence interval around it. This second purpose requires a larger sample size than does merely seeing if the regression coefficient is different from zero. 

**A power analysis = study plan**

- Can be used to estimate the "minimum" sample size required for an experiment, given a desired significance level, effect size, and statistical power (many recommendations fall between 0.8 and 0.9). For example, how many locations (sample size) do we need to detect the frog mortality decrease of 10% (effect size) from 2019 to 2020 with a statistical power of 80% and a significance level of 0.05? **But you don't want to just sample the "minimum" sample size, you want add a few more to give yourself some "wiggle-room" just in case.**

- Note: Post-experiment power calculations should <font size="2" color="red"> **NOT**</font> be used to aid in the interpretation of the experimental results. Details see (Hoenig and Heisey 2001).

**Limitations of power analysis:** 

- If you change the methodology used to collect the data or change the statistical procedure, you will mostly likely have to re-do power analysis. 
- A standard power analysis gives you a “best case scenario” estimate of the necessary number of subjects needed to detect the effect. In most cases, this “best case scenario” is based on assumptions and educated guesses.  If any of these assumptions or guesses are incorrect, you may have less power than you need to detect the effect. 
- You usually get a range of numbers you need, rather than a precise number. For example, if you do not know what the standard deviation of your outcome measure will be, you guess at this value, run the power analysis and get X number of subjects. 
- The number of statistical tests that you intend to conduct will influence your necessary sample size:  the more tests that you want to run, the more subjects that you will need. 

<p>&nbsp;</p>

#### 3.2 How to do power analysis? (10 min)

1. Think through your research project and your purpose of the study (coefficient different from 0, precise point estimate, etc.). Specify your null hypothesis. 

2. List your statistical analysis. Try to know as much as possible about the statistical techniques, such as assumptions, types of data, whether there is alpha inflation issue, etc. 

3. Decide on the effect size (or a range of effect sizes), significance level (alpha), and power. How to guestimate the values? Do a literature review, a pilot study and using Cohen's recommendations. Here is Cohen's recommendations for the effect size: 
    - Small effect:  1% of the variance; d = 0.2 (too small to detect other than statistically) 
    - Medium effect:  6% of the variance; d = 0.5 (apparent with careful observation)
    - Large effect: at least 15% of the variance; d = 0.8 (apparent with a superficial glance; unlikely to be the focus of research because it is too obvious)

4. Coding. 


**Example 1**

Recall our research question: Has frog mortality changed in current COVID-19 situation in Ontario compared to before?

1. Null hypothesis: the mean frog mortality in Ontario in 2019 and 2020 is the same.

2. Statistical test: paired t-test (because two sample sets are the same subjects but two measurements of mortality). 
Null hypothesis: the true mean difference is zero. 
Assumptions: 
    - The dependent variable must be continuous (interval/ratio).
    - The observations are independent of one another.
    - The dependent variable should be approximately normally distributed.
    - The dependent variable should not contain any outliers.

3. I want to test how many individuals I need with: 
    - small effect size, alpha at 0.05, power at 0.8
    - small effect size, alpha at 0.01, power at 0.8
    - medium effect size, alpha at 0.05, power at 0.9 

4. Coding.


**Manual coding** 

Calculating power is quite similar to the process of calculating p-value. Ying recommends you to read *Statistical Power Analysis for the Behavioral Sciences* by Cohen if you are interested in the detailed calculation process. Teaching the mathematics is beyond our ability and also beyond the purpose of this tutorial. Here is a small tutorial on the t-test to give you some flavors: https://www.cyclismo.org/tutorial/R/power.html#calculating-many-powers-from-a-t-distribution. 

Despite of not covering it for this tutorial, we want to **emphasize** on <font size="3" color="red"> UNDERSTANDING THE LOGIC AND ASSUMPTIONS OF THE STATISTICS </font>. You may not know how to cook qie-xiang (an eggplant dish cooked with "10 chickens" in the Chinese classic novel *The Story of the Stone*), but you are probably able to tell whether eggplants are nicely cooked. So as the statistics. You may forget or not know the exact equations, but you should understand it such that you are able to tell whether it is used appropriately. 

What matters? Ying says the distribution, in other words, the statistical method. 

```{r,echo=FALSE,warning=FALSE,message=FALSE}
curve(dnorm(x,24,1), xlim=c(20,28), main="Normal density",axes=FALSE,xlab="",ylab="",col="blue",lwd=5)
curve(dnorm(x,23,1),add=TRUE,col="turquoise",lwd=5)
axis(1,at=c(20,21,22,23,24,25,26,27,28))

# define shaded region
from.z1 <- 20
to.z1 <- qnorm(.025,mean=24,sd=1)

from.z2 <- qnorm(.975,mean=24,sd=1)
to.z2 <- 28

S.x  <- c(from.z1, seq(from.z1, to.z1, 0.01), to.z1)
S.y1  <- c(0, dnorm(seq(from.z1, to.z1, 0.01),24,1), 0)
S.y2  <- c(0, dnorm(seq(from.z1, to.z1, 0.01),23,1), 0)
polygon(S.x,S.y2, col="turquoise",border=NA)
polygon(S.x,S.y1, col="blue",border=NA)

S.x  <- c(from.z2, seq(from.z2, to.z2, 0.01), to.z2)
S.y1  <- c(0, dnorm(seq(from.z2, to.z2, 0.01),24,1), 0)
S.y2  <- c(0, dnorm(seq(from.z2, to.z2, 0.01),23,1), 0)
polygon(S.x,S.y1, col="blue",border=NA)
polygon(S.x,S.y2, col="turquoise",border=NA)

abline(v=to.z1,lty="dashed")
abline(v=from.z2,lty="dashed")

text(x=25,y=0.37,labels=expression(paste(mu, "=24")),col="darkblue")
text(x=24.2,y=0.3,labels=expression(paste(mu, "=23")),col="turquoise")
text(x=21.8,y=0.37,labels=expression(paste(alpha, "=0.025")))
text(x=26,y=0.37,labels=expression(paste(alpha, "=0.025")))
```


**Package pwr:**

- Official release on CRAN: http://cran.r-project.org/web/packages/pwr/ 
- Github link: https://github.com/heliosdrm/pwr

```{r}
library(pwr)
# 1. small effect size, alpha at 0.05, power at 0.8
# n is the sample size, d is the effect size, Cohen's d = mean/SD for paired t-test, sig.level is alpha, type indicates a two-sample t-test, one-sample t-test or paired t-test, alternative hypothesis is two.sided (default)
pwr.t.test(d = 0.2, sig.level = 0.05, power = 0.8, type = c("paired"),alternative="two.sided")
```

Now let's visualize how power change with sample size. 

```{r,echo=FALSE}
# plot color names: https://www.maplesoft.com/support/help/Maple/view.aspx?path=plot/colornames 
samplesizes <- seq(from=10,to=300,by=10)
power.samplesizes <- pwr.t.test(n=samplesizes,sig.level=0.05,d=0.2,type=c("paired"),alternative="two.sided")$power
plot(samplesizes,
     power.samplesizes,
     xlim=c(0,300),
     xlab="Sample size",
     ylab="Expected power",
     ylim=c(0,1),
     type="b",
     col="darkorange",
     lwd=5,axes=FALSE)
axis(1,at=c(0,50,100,150,200,250,300))
axis(2,at=c(0,0.25,0.5,0.75,1),labels=paste(c(0,25,50,75,100),"%"))
text(x=50,y=0.9,labels="sig.level=0.05",col="darkblue")
text(x=50,y=0.8,labels="effect.size=0.2",col="darkblue")
```

Now let's visualize how sample size changes with effect size. 

```{r,echo=FALSE}
effectsizes <- seq(from=0.1,to=0.9,by=0.1)
samplesize.effectsizes.08 <-sapply(effectsizes, function(d){pwr.t.test(sig.level=0.05,d=d,power=0.8,type=c("paired"),alternative="two.sided")$n}) # We use sapply because pwr.t.test doesn't know how to deal with a sequence of effect sizes 
samplesize.effectsizes.09 <-sapply(effectsizes, function(d){pwr.t.test(sig.level=0.05,d=d,power=0.9,type=c("paired"),alternative="two.sided")$n})
plot(effectsizes,
     samplesize.effectsizes.08,
     xlim=c(0,1),
     xlab="Effect size",
     ylab="Required sample size",
     type="b",
     col="darkblue",
     lwd=5,axes=FALSE)
lines(effectsizes,samplesize.effectsizes.09,col="turquoise",lwd=5,type="b")
axis(1,at=c(0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1))
axis(2,at=c(1000,500,350,100,50,10,0))
legend(x="topright",lwd=5,bty="n",legend=c("power=0.8","power=0.9"),col=c("darkblue","turquoise"))
text(x=0.9,y=650,labels="sig.level=0.05",col="darkorange")
```

<font size="3" color="blue">*Exercise:* *Try the other 2 scenarios. How many individuals do you need with (1) small effect size, alpha at 0.01, power at 0.8? (2) medium effect size, alpha at 0.05, power at 0.9?*</font>


<p>&nbsp;</p>
**Example 2**

Recall our research question: Has frog mortality changed in current COVID-19 situation in Ontario compared to before?

1. Null hypothesis: the frog mortality rate does not correlate with traffic in Ontario (correlation coefficient r=0).
Alternative hypothesis: the frog mortality rate increase with traffic in Ontario (correlation coefficient r>0).

2. Statistical analysis: correlation test. Null hypothesis is the true population correlation coefficient is 0. Assumptions: 
    - Both variables should be normally distributed. 
    - Both variables are continous (interval or ratio). 
    - Two vairables are linearly related.

3. I want to test how many individuals I need with: 
    - small effect size, alpha at 2, power at 0.8 
    - small effect size, alpha at 0.01, power at 0.8
    - medium effect size, alpha at 0.05, power at 0.9 

4. Coding.


```{r}
# n is the sample size and r is the correlation, notice our alternative is greater
pwr.r.test(r = 0.2, sig.level = 0.05, power = 0.8, alternative="greater")

pwr.r.test(r = 0.2, sig.level = 0.01, power = 0.8, alternative="greater")

pwr.r.test(r = 0.35, sig.level = 0.05, power = 0.8, alternative="greater")
```


<p>&nbsp;</p>
<font size="3" color="cornflowerBlue">*It's your show time!*</font>
<font size="3" color="violet">*In reality, one would expect frog mortality is affected by many factors, such as predation and climate. Thus one would expect the mean mortality rate changes across years. That is to say, with or without the pandemic frog mortality rate might change in 2020 as a natural pattern. We are interested in whether frog mortality change unusually in 2020 compared to the past 10 years and if so, whether it is because of the pandemic lockdown. **Ying** raised a hypothesis that frog mortality rate change in 2020 deviates from the changing pattern in the past 10 years. Do you think this is a good hypothesis? How would you design your study and do statistical analyses?*</font>
















<p>&nbsp;</p>

#### 3.3 Alternative to p value - Estimation stats and bootstraping (Ying)

<font size="2" color="red"> *The following materials are prepared by Ying upon her understanding, which means they can be flawed.* </font> 

<font size="2" color="red"> *Ying highly recommends you to read authoritative books and papers for learning. Here are some:* </font> 

- <font size="2" color="gray">*Coe, R. 2002. It's the effect size, stupid! What effect size is and why it is important. Annual Conference of the British Educational Research Association; University of Exeter, England.*</font>
- <font size="2" color="gray">*Sullivan, G. M., and R. Feinn. 2012. Using effect size — or why the p value is not enough. Journal of Graduate Medical Education 4(3): 279-282*</font>
- <font size="2" color="gray"> *Wasserstein, R. L., A. L. Schirm, and N. A. Lazar. 2019. Moving to a world beyond “p < 0.05”. The American Statistician, 73:1-19.*</font>
- <font size="2" color="gray"> *Halsey, L. G. 2019. The reign of the p-value is wver: What alternative analyses could we employ to fill the power vacuum? Biology Letters 15: 20190174*</font>
- <font size="2" color="gray"> *Amrhein, V., S. Greenland, and B. Mcshane. 2019. Retire statistical significance. Nature 567:305-307.*</font>


Problems of p-value and statistically significant:

- Not informative of observed data 
- 


Alternatives:

- Bayesian analysis 
- 



**Package dabestr** 

- Website: https://www.estimationstats.com/?fbclid=IwAR0d6nqcf3IbNgSb772R3TnzL4Nv7AtQFmLNToBvISmQmEMFyyVF6FdJ-lc#/background 
- Official release on CRAN: https://cran.r-project.org/web/packages/dabestr/index.html 
- Github link: https://github.com/ACCLAB/dabestr 

```{r}
# Estimation stats: https://www.estimationstats.com/?fbclid=IwAR0d6nqcf3IbNgSb772R3TnzL4Nv7AtQFmLNToBvISmQmEMFyyVF6FdJ-lc#/background 
# Codes are from https://cran.r-project.org/web/packages/dabestr/vignettes/using-dabestr.html 
library(dplyr)

set.seed(54321)

N = 10
N = 40
N = 100
c1 <- rnorm(N, mean = 100, sd = 25)
c2 <- rnorm(N, mean = 100, sd = 50)
g1 <- rnorm(N, mean = 120, sd = 25)
g2 <- rnorm(N, mean = 80, sd = 50)
g3 <- rnorm(N, mean = 100, sd = 12)
g4 <- rnorm(N, mean = 100, sd = 50)
gender <- c(rep('Male', N/2), rep('Female', N/2))
dummy <- rep("Dummy", N)
id <- 1: N

wide.data <- 
  tibble::tibble(
    Control1 = c1, Control2 = c2,
    Group1 = g1, Group2 = g2, Group3 = g3, Group4 = g4,
    Dummy = dummy,
    Gender = gender, ID = id)


my.data   <- 
  wide.data %>%
  tidyr::gather(key = Group, value = Measurement, -ID, -Gender, -Dummy)


library(dabestr)

two.group.unpaired <- 
  my.data %>%
  dabest(Group, Measurement, 
         # The idx below passes "Control" as the control group, 
         # and "Group1" as the test group. The mean difference
         # will be computed as mean(Group1) - mean(Control1).
         idx = c("Control1", "Group1"), 
         paired = FALSE)

two.group.unpaired 

plot(two.group.unpaired, color.column = Gender)

```











<<<<<<< HEAD
<p>&nbsp;</p>
<p>&nbsp;</p>

#### dplyr (20 min) (Ying):

1. Import Data
2. How to change data into what you want
3. how to get rid of NA in your data
4. how to make a new collumn in your data
5. how to change the orientation and layout of your data (Hayden)
6. double checking data sets for spelling mistakes
7. piping and filtering/sort data
8. creating new data frames
9. how to make a small program (for loop, if statements, make your own functions) (Hayden to help)



#### ggplot (20min) (Ying and Hayden):

```{r}
library(ggplot2)
```


1. qplot 

```{r}
#qplot(Species, Frog_Mort, Data1, geom = "auto")
```

2. plotting basic functions in ggplot using geoms
    + boxplot plot: mortality vs species 
    
```{r}
Species_Mort <- ggplot(Data1, aes(x=Year, y=Frog_Mort))+
  geom_boxplot()
Species_Mort


```
    
3. adding trend lines 
    + scatter plot + linear model: mortality vs traffic 
4. adding multiple data sets to same graph
5. anotate text and axises
6. changin colour schemes and shapes
    + fill color of different frog species, different locations
7. change theme of a plot 
8. plotting models 
9. confidence intervals and standard error bars
    + linear model: mortality vs traffic, mortality vs food supply in ontario 
10. how to add multiple plots to one graph space



=======
>>>>>>> 18adb922f301d7ddbdd76b8359af08b8566b8720


